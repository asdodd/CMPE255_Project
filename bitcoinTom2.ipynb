{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net using BitcoinHeistData\n",
    "\n",
    "Inspired by: https://www.freecodecamp.org/news/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('BitcoinHeistData.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into our input features (X) and the label (Y) we wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X=df.iloc[:, 3:9]\n",
    "Ytmp=df.iloc[:, -1]\n",
    "\n",
    "print(X.shape)\n",
    "print(Ytmp.shape)\n",
    "\n",
    "print(X)\n",
    "print(Ytmp)\n",
    "\n",
    "# Convert the label to true false for simplicity\n",
    "Y = Ytmp.str.contains('white')\n",
    "print(Y)\n",
    "numWhite = np.count_nonzero(Y)\n",
    "print(numWhite/len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data so one feature does not dominate.\n",
    "Use a min-max scaler from scikit-learn which scales our data to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset for a validation set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Keras to build the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Sequential model, which means that we merely need to describe the layers above in sequence. Our neural network has three layers:\n",
    "\n",
    "- Hidden layer 1: 30 neurons, ReLU activation\n",
    "- Hidden layer 2: 30 neurons, ReLU activation\n",
    "- Output Layer: 1 neuron, Sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(6,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our architecture specified, we need to find the best numbers for it. Before we start our training, we have to configure the model by\n",
    "- Telling it what algorithm you want to use to do the optimization (we'll use stochastic gradient descent)\n",
    "- Telling it what loss function to use (for binary classification, we will use binary cross entropy)\n",
    "- Telling it what other metrics you want to track apart from the loss function (we want to track accuracy as well)\n",
    "\n",
    "We do so below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the data is pretty straightforward and requires us to write one line of code. The function is called 'fit' as we are fitting the parameters to the data. We specify:\n",
    "- what data we are training on, which is X_train and Y_train\n",
    "- the size of our mini-batch \n",
    "- how long we want to train it for (epochs)\n",
    "- what our validation data is so that the model will tell us how we are doing on the validation data at each point.\n",
    "\n",
    "This function will output a history, which we save under the variable hist. We'll use this variable a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually train with 100 epochs, but each epoch same here (issue?)\n",
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=5,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating our data on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Loss andÂ Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to visualize the training loss and the validation loss like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(hist.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the training accuracy and the validation accuracy like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], '-o')\n",
    "plt.plot(hist.history['val_accuracy'], '-x')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.ylim(0.95, 1)\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(type(X_test))\n",
    "print(type(Y_test))\n",
    "print(hist.params)\n",
    "print(hist.history.keys())\n",
    "\n",
    "# Look at results where label is not 'white'.  Do we ever predict a heist?\n",
    "#print(X_test[1,:])\n",
    "#fooY = Y_test.to_numpy()\n",
    "#print(fooY[1])\n",
    "#idxWhite = np.where(Y_test)[0]\n",
    "#print(type(idxWhite))\n",
    "#print(idxWhite.shape)\n",
    "#model.evaluate(X_test[idxWhite,:], Y_test[idxWhite])\n",
    "\n",
    "#for i in range(X_test):\n",
    "#    xt = X_test[i]\n",
    "#    yt = Y_test[i]\n",
    "#    model.evaluate(xt, yt)[1]\n",
    "#    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\\n\",\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# evaluate the model\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "#n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "#print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "ynew = model.predict(X_test)\n",
    "ynew.astype(int)\n",
    "print(ynew)\n",
    "print(np.unique(ynew))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ynew))\n",
    "print(type(Y_test))\n",
    "print(type(ynew[0]))\n",
    "print(type(Y_test[0]))\n",
    "print(ynew.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "Y_test_np = Y_test.to_numpy()\n",
    "print(ynew[3])\n",
    "print(Y_test_np[3])\n",
    "\n",
    "numCorrect=0\n",
    "numIncorrect=0\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for idx in range(0, ynew.size-1):\n",
    "    if ynew[idx] == Y_test_np[idx]:\n",
    "        numIncorrect = numIncorrect + 1\n",
    "    else:\n",
    "        numCorrect = numCorrect + 1\n",
    "    if ynew[idx] == True and Y_test_np[idx] == True:\n",
    "        tp = tp + 1\n",
    "    elif ynew[idx] == False and Y_test_np[idx] == False:\n",
    "        tn = tn + 1\n",
    "    elif ynew[idx] == True and Y_test_np[idx] == False:\n",
    "        fp = fp + 1\n",
    "    elif ynew[idx] == False and Y_test_np[idx] == True:\n",
    "        fn = fn + 1\n",
    "\n",
    "print(\"numCorrect   =\" + str(numCorrect))\n",
    "print(\"numIncorrect =\" + str(numIncorrect))\n",
    "print(\"Accuracy     =\" + str(numCorrect/ynew.size))\n",
    "\n",
    "print(\"tp   =\" + str(tp))\n",
    "print(\"tn   =\" + str(tn))\n",
    "print(\"fp   =\" + str(fp))\n",
    "print(\"fn   =\" + str(fn))\n",
    "\n",
    "print(\"precision=\" + str(tp/(tp+fp)))\n",
    "print(\"recall   =\" + str(tp/(tp+fn)))\n",
    "print(\"F1       =\" + str(2*tp/(2*tp+fp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
